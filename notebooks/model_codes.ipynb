{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DCA First Try\n",
    "# Define the model\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = GlobalAveragePooling2D()(x)\n",
    "    attention = Dense(attention_units, activation='relu')(attention)\n",
    "    attention = Dense(attention_units, activation='sigmoid')(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(attention)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Self Attention\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    # attention = GlobalAveragePooling2D()(x)\n",
    "    attention = Attention()([x, x])\n",
    "    attention = Dense(attention_units, activation='relu')(attention)\n",
    "    attention = Dense(attention_units, activation='sigmoid')(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(attention)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze and Excitation (SE)\n",
    "\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    # Squeeze operation\n",
    "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
    "\n",
    "    # Excitation operation\n",
    "    excitation = Dense(input_tensor.shape[-1] // ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(input_tensor.shape[-1], activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 1, -1))(excitation)\n",
    "\n",
    "    # Scale the input tensor\n",
    "    scaled_input = Multiply()([input_tensor, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Attention mechanism (Squeeze-and-Excite)\n",
    "    attention = squeeze_excite_block(x, ratio=8)\n",
    "\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(attention)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SE Extra \n",
    "\n",
    "# Define the model\n",
    "\n",
    "\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    # Squeeze operation\n",
    "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
    "\n",
    "    # Excitation operation\n",
    "    excitation = Dense(input_tensor.shape[-1] // ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(input_tensor.shape[-1], activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 1, -1))(excitation)\n",
    "\n",
    "    # Scale the input tensor\n",
    "    scaled_input = Multiply()([input_tensor, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Attention mechanism (Squeeze-and-Excite)\n",
    "    attention = squeeze_excite_block(x, ratio=8)\n",
    "\n",
    "    # Additional Convolutional layers\n",
    "    x = Conv2D(256, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01))(attention)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    dropout_rate=0.5\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM attention\n",
    "\n",
    "# Define the model\n",
    "def convolutional_block_attention_module(input_tensor, ratio=8):\n",
    "    # Channel attention\n",
    "    channel_avg = GlobalAveragePooling2D()(input_tensor)\n",
    "    channel_max = GlobalAveragePooling2D()(input_tensor)\n",
    "    channel_shared = Dense(input_tensor.shape[-1] // ratio, activation='relu', kernel_regularizer=regularizers.l2(0.01))(channel_avg) + \\\n",
    "                     Dense(input_tensor.shape[-1] // ratio, activation='relu', kernel_regularizer=regularizers.l2(0.01))(channel_max)\n",
    "    channel_attention = Dense(input_tensor.shape[-1], activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(channel_shared)\n",
    "    channel_attention = Reshape((1, 1, -1))(channel_attention)\n",
    "\n",
    "    # Spatial attention\n",
    "    spatial_avg = Reshape((1, 1, input_tensor.shape[-1]))(GlobalAveragePooling2D()(input_tensor))\n",
    "    spatial_max = Reshape((1, 1, input_tensor.shape[-1]))(GlobalAveragePooling2D()(input_tensor))\n",
    "    spatial_shared = Add()([Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(spatial_avg),\n",
    "                           Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(spatial_max)])\n",
    "    spatial_attention = Activation('sigmoid')(spatial_shared)\n",
    "\n",
    "    # Combine channel and spatial attention\n",
    "    attention = Multiply()([input_tensor, channel_attention, spatial_attention])\n",
    "\n",
    "    return attention\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Apply CBAM\n",
    "    # x = convolutional_block_attention_module(x)\n",
    "\n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Apply CBAM\n",
    "    x = convolutional_block_attention_module(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Additional Convolutional layers\n",
    "    x = Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    # Apply CBAM\n",
    "    x = convolutional_block_attention_module(x)\n",
    "\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    dropout_rate=0.5\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Attention from SE\n",
    "# Define the model\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(filters = 64, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(input_tensor)\n",
    "    maxpool1 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "    # attn1 = Attention(maxpool1, ))\n",
    "\n",
    "    conv2 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool2)\n",
    "    maxpool3 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv3)\n",
    "\n",
    "    attn = Attention(attention_units)([maxpool3, maxpool3])\n",
    "\n",
    "    flattened = Flatten()(attn)\n",
    "\n",
    "    fc1 = Dense(units = 512, activation = 'relu')(flattened)\n",
    "    drop1 = Dropout(rate = 0.5)(fc1)\n",
    "\n",
    "    final = Dense(units = num_classes,activation='sigmoid')(drop1)\n",
    "\n",
    "    model = Model(inputs=input_tensor, outputs=final)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SE with VGG16\n",
    "\n",
    "# Define the model\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    # Squeeze operation\n",
    "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
    "\n",
    "    # Excitation operation\n",
    "    excitation = Dense(input_tensor.shape[-1] // ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(input_tensor.shape[-1], activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 1, -1))(excitation)\n",
    "\n",
    "    # Scale the input tensor\n",
    "    scaled_input = Multiply()([input_tensor, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    # Load pre-trained VGG16 model without top (fully connected) layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get output tensor of the last VGG16 layer\n",
    "    x = base_model.output\n",
    "\n",
    "    # Dilated CNN block with attention\n",
    "    x = Conv2D(256, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    # Additional Convolutional layers if needed\n",
    "    x = Conv2D(512, (3, 3), dilation_rate=dilation_rate, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    # Global average pooling and dense layers\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA from SE on VGG16\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    # Load pre-trained VGG16 model without top (fully connected) layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get output tensor of the last VGG16 layer\n",
    "    x = base_model.output\n",
    "\n",
    "    conv1 = Conv2D(filters = 64, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(x)\n",
    "    maxpool1 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "    # attn1 = Attention(maxpool1, ))\n",
    "\n",
    "    conv2 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool2)\n",
    "    maxpool3 = MaxPooling2D(pool_size = (1, 1), strides=(2, 2))(conv3)\n",
    "\n",
    "    attn = Attention(attention_units)([maxpool3, maxpool3])\n",
    "\n",
    "    flattened = Flatten()(attn)\n",
    "\n",
    "    fc1 = Dense(units = 512, activation = 'relu')(flattened)\n",
    "    drop1 = Dropout(rate = 0.5)(fc1)\n",
    "\n",
    "    final = Dense(units = num_classes,activation='sigmoid')(drop1)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=final)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDataGenerator' object has no attribute 'next'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[43mdatagen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m()\n",
      "\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDataGenerator' object has no attribute 'next'"
     ]
    }
   ],
   "source": [
    "# SA from SE on ResNet50\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    # Load pre-trained VGG16 model without top (fully connected) layers\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get output tensor of the last VGG16 layer\n",
    "    x = base_model.output\n",
    "\n",
    "    conv1 = Conv2D(filters = 64, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(x)\n",
    "    maxpool1 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv1)\n",
    "\n",
    "    # attn1 = Attention(maxpool1, ))\n",
    "\n",
    "    conv2 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool2)\n",
    "    maxpool3 = MaxPooling2D(pool_size = (1, 1), strides=(2, 2))(conv3)\n",
    "\n",
    "    attn = Attention(attention_units)([maxpool3, maxpool3])\n",
    "\n",
    "    flattened = Flatten()(attn)\n",
    "\n",
    "    fc1 = Dense(units = 512, activation = 'relu')(flattened)\n",
    "    drop1 = Dropout(rate = 0.5)(fc1)\n",
    "\n",
    "    final = Dense(units = num_classes,activation='sigmoid')(drop1)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=final)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SA6L on VGG16\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    # Load pre-trained VGG16 model without top (fully connected) layers\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get output tensor of the last VGG16 layer\n",
    "    x = base_model.output\n",
    "\n",
    "    conv1 = Conv2D(filters = 64, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(x)\n",
    "    maxpool1 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2), padding='same')(conv1)\n",
    "\n",
    "    # attn1 = Attention(maxpool1, ))\n",
    "\n",
    "    conv2 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2), padding='same')(conv2)\n",
    "\n",
    "    conv3 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool2)\n",
    "    maxpool3 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2), padding='same')(conv3)\n",
    "\n",
    "    conv4 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool3)\n",
    "    maxpool4 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2), padding='same')(conv4)\n",
    "\n",
    "    conv5 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool4)\n",
    "    maxpool5 = MaxPooling2D(pool_size = (2, 2), strides=(2, 2), padding='same')(conv5)\n",
    "\n",
    "    conv6 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool5)\n",
    "    maxpool6 = MaxPooling2D(pool_size = (1, 1), strides=(2, 2))(conv6)\n",
    "\n",
    "    attn = Attention(attention_units)([maxpool6, maxpool6])\n",
    "\n",
    "    flattened = Flatten()(attn)\n",
    "\n",
    "    fc1 = Dense(units = 512, activation = 'relu')(flattened)\n",
    "    drop1 = Dropout(rate = 0.5)(fc1)\n",
    "\n",
    "    final = Dense(units = num_classes,activation='sigmoid')(drop1)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=final)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAP\n",
    "\n",
    "# Define the model\n",
    "def squeeze_excite_block(input_tensor, ratio=8):\n",
    "    # Squeeze operation\n",
    "    squeeze = GlobalAveragePooling2D()(input_tensor)\n",
    "\n",
    "    # Excitation operation\n",
    "    excitation = Dense(input_tensor.shape[-1] // ratio, activation='relu')(squeeze)\n",
    "    excitation = Dense(input_tensor.shape[-1], activation='sigmoid')(excitation)\n",
    "    excitation = Reshape((1, 1, -1))(excitation)\n",
    "\n",
    "    # Scale the input tensor\n",
    "    scaled_input = Multiply()([input_tensor, excitation])\n",
    "\n",
    "    return scaled_input\n",
    "\n",
    "def convolutional_block_attention_module(input_tensor, ratio=8):\n",
    "    # Channel attention\n",
    "    channel_avg = GlobalAveragePooling2D()(input_tensor)\n",
    "    channel_max = GlobalAveragePooling2D()(input_tensor)\n",
    "    channel_shared = Dense(input_tensor.shape[-1] // ratio, activation='relu', kernel_regularizer=regularizers.l2(0.01))(channel_avg) + \\\n",
    "                     Dense(input_tensor.shape[-1] // ratio, activation='relu', kernel_regularizer=regularizers.l2(0.01))(channel_max)\n",
    "    channel_attention = Dense(input_tensor.shape[-1], activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(channel_shared)\n",
    "    channel_attention = Reshape((1, 1, -1))(channel_attention)\n",
    "\n",
    "    # Spatial attention\n",
    "    spatial_avg = Reshape((1, 1, input_tensor.shape[-1]))(GlobalAveragePooling2D()(input_tensor))\n",
    "    spatial_max = Reshape((1, 1, input_tensor.shape[-1]))(GlobalAveragePooling2D()(input_tensor))\n",
    "    spatial_shared = Add()([Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(spatial_avg),\n",
    "                           Dense(1, activation='relu', kernel_regularizer=regularizers.l2(0.01))(spatial_max)])\n",
    "    spatial_attention = Activation('sigmoid')(spatial_shared)\n",
    "\n",
    "    # Combine channel and spatial attention\n",
    "    attention = Multiply()([input_tensor, channel_attention, spatial_attention])\n",
    "\n",
    "    return attention\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    # Load pre-trained VGG16 model without top (fully connected) layers\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "\n",
    "    # Freeze pre-trained layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Get output tensor of the last VGG16 layer\n",
    "    x = base_model.output\n",
    "\n",
    "    conv1 = Conv2D(filters = 64, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(x)\n",
    "    conv2 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(conv1)\n",
    "    # normalize = BatchNormalization()(conv2)\n",
    "    conv3 = Conv2D(filters = 128, kernel_size = (3, 3), activation = 'relu', padding='same')(conv2)\n",
    "    # maxpool3 = MaxPooling2D(pool_size = (2, 2), strides=(1, 1))(conv3)\n",
    "\n",
    "    conv4 = Conv2D(filters = 256, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(conv3)\n",
    "    conv5 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(conv4)\n",
    "    normalize = BatchNormalization()(conv5)\n",
    "    conv6 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(conv5)\n",
    "    maxpool6 = MaxPooling2D(pool_size = (2, 2), strides=(1, 1))(conv6)\n",
    "\n",
    "    conv7 = Conv2D(filters = 256, dilation_rate = dilation_rate, kernel_size = (3, 3), activation = 'relu', padding='same')(maxpool6)\n",
    "    conv8 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(conv7)\n",
    "    normalize = BatchNormalization()(conv8)\n",
    "    conv9 = Conv2D(filters = 256, kernel_size = (3, 3), activation = 'relu', padding='same')(normalize)\n",
    "    maxpool9 = MaxPooling2D(pool_size = (2, 2), strides=(1, 1))(conv9)\n",
    "\n",
    "\n",
    "    attn = Attention(attention_units)([maxpool9, maxpool9])\n",
    "    # attn = convolutional_block_attention_module(maxpool9)\n",
    "\n",
    "    flattened = Flatten()(attn)\n",
    "\n",
    "    fc1 = Dense(units = 512, activation = 'relu')(flattened)\n",
    "    # drop1 = Dropout(rate = 0.5)(fc1)\n",
    "\n",
    "    final = Dense(units = num_classes,activation='sigmoid')(fc1)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=final)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
