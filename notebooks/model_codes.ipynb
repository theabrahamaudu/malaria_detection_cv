{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DCA First Try\n",
    "# Define the model\n",
    "\n",
    "def build_dilated_cnn_with_attention(input_shape, num_classes, dilation_rate, attention_units):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "\n",
    "    # Dilated CNN block\n",
    "    x = Conv2D(64, (3, 3), dilation_rate=dilation_rate, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), dilation_rate=dilation_rate, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "    # Attention mechanism\n",
    "    attention = GlobalAveragePooling2D()(x)\n",
    "    attention = Dense(attention_units, activation='relu')(attention)\n",
    "    attention = Dense(attention_units, activation='sigmoid')(attention)\n",
    "    attention = Multiply()([x, attention])\n",
    "\n",
    "    # Global average pooling and final dense layer\n",
    "    x = GlobalAveragePooling2D()(attention)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    output_tensor = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
